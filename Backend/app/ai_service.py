"""
AI Service for Document Analysis using Google Gemini
"""

from google import genai
from google.genai import types
from PIL import Image
import json
import re
import base64
import io
from typing import Dict, Any, Optional
import os

# Person Info Extraction Prompt - For CCCD/ID Cards/Driver License
PERSON_INFO_EXTRACTION_PROMPT = """You are an expert at extracting personal information from Vietnamese ID cards (CCCD), Driver Licenses, and similar documents.

Your task is to extract personal information from this document image and return it in JSON format.

CRITICAL RULES:
1. Extract ONLY information that is CLEARLY VISIBLE in the document
2. DO NOT invent or guess any information
3. Return null for fields that are not present
4. Keep Vietnamese text as-is (DO NOT translate)
5. Extract dates in DD/MM/YYYY format

SUPPORTED DOCUMENT TYPES:
- CCCD (CƒÉn c∆∞·ªõc c√¥ng d√¢n) - Vietnamese ID Card
- CMND (Ch·ª©ng minh nh√¢n d√¢n) - Old ID Card
- B·∫±ng l√°i xe (Driver License)
- H·ªô chi·∫øu (Passport)
- S·ªï h·ªô kh·∫©u (Household Registration)

JSON OUTPUT FORMAT:
{
  "fullName": "H·ªç v√† t√™n ƒë·∫ßy ƒë·ªß | null",
  "dateOfBirth": "DD/MM/YYYY | null",
  "gender": "Nam | N·ªØ | null",
  "idNumber": "S·ªë CCCD/CMND/B·∫±ng l√°i | null",
  "address": "ƒê·ªãa ch·ªâ ƒë·∫ßy ƒë·ªß | null",
  "phone": "S·ªë ƒëi·ªán tho·∫°i (n·∫øu c√≥) | null",
  "email": "Email (n·∫øu c√≥) | null",
  "placeOfOrigin": "Qu√™ qu√°n | null",
  "nationality": "Qu·ªëc t·ªãch | null",
  "issueDate": "Ng√†y c·∫•p DD/MM/YYYY | null",
  "expiryDate": "Ng√†y h·∫øt h·∫°n DD/MM/YYYY | null",
  "documentType": "CCCD | CMND | Driver License | Passport | etc."
}

FIELD EXTRACTION RULES:

fullName:
- Extract from "H·ªç v√† t√™n" / "Name" field
- Keep Vietnamese characters (√™, √¥, ∆°, ƒÉ, etc.)
- Capitalize properly: "NGUY·ªÑN VƒÇN A" ‚Üí "Nguy·ªÖn VƒÉn A"

dateOfBirth:
- Extract from "Ng√†y sinh" / "Date of birth"
- Format: DD/MM/YYYY
- Example: "01/01/1990"

gender:
- Extract from "Gi·ªõi t√≠nh" / "Sex"
- Return "Nam" or "N·ªØ" (Vietnamese)
- If M/Male ‚Üí "Nam", if F/Female ‚Üí "N·ªØ"

idNumber:
- Extract from "S·ªë" field (CCCD/CMND number)
- Or "S·ªë b·∫±ng l√°i" (Driver license number)
- Keep exactly as shown (no spaces, dashes preserved)
- Example: "001234567890" or "079123456789"

address:
- Extract from "N∆°i th∆∞·ªùng tr√∫" / "Place of residence"
- Full address with street, ward, district, city
- Example: "123 Nguy·ªÖn Hu·ªá, Ph∆∞·ªùng B·∫øn Ngh√©, Qu·∫≠n 1, TP.HCM"

phone:
- Extract if visible on document (not always present)
- Format: Keep as shown (with or without spaces)
- Example: "0901234567"

email:
- Extract if visible (rarely present on ID cards)

placeOfOrigin:
- Extract from "Qu√™ qu√°n" / "Place of origin"
- Example: "H√† N·ªôi"

nationality:
- Usually "Vi·ªát Nam" for Vietnamese ID
- Extract from "Qu·ªëc t·ªãch" field

issueDate:
- Extract from "Ng√†y c·∫•p" / "Date of issue"
- Format: DD/MM/YYYY

expiryDate:
- Extract from "C√≥ gi√° tr·ªã ƒë·∫øn" / "Valid until"
- Format: DD/MM/YYYY
- May be "Kh√¥ng th·ªùi h·∫°n" (No expiry) ‚Üí return "Kh√¥ng th·ªùi h·∫°n"

documentType:
- Auto-detect from document appearance
- Values: "CCCD" | "CMND" | "Driver License" | "Passport" | "Household Registration"

IMPORTANT:
- Return ONLY valid JSON (no markdown, no explanations)
- All text values must be properly escaped
- Use null (not "null" string) for missing fields
- Preserve Vietnamese diacritics exactly

Now extract personal information from this document:"""

# Vehicle Info Extraction Prompt - For Vehicle Registration (C√† v·∫πt xe)
VEHICLE_INFO_EXTRACTION_PROMPT = """You are an expert at extracting vehicle information from Vietnamese vehicle registration documents (Gi·∫•y ƒëƒÉng k√Ω xe / C√† v·∫πt).

Your task is to extract vehicle information from this document image and return it in JSON format.

CRITICAL RULES:
1. Extract ONLY information that is CLEARLY VISIBLE in the document
2. DO NOT invent or guess any information
3. Return null for fields that are not present
4. Keep Vietnamese text as-is (DO NOT translate)
5. Extract dates in DD/MM/YYYY format

SUPPORTED DOCUMENT TYPES:
- Gi·∫•y ƒëƒÉng k√Ω xe √¥ t√¥ (Car registration)
- Gi·∫•y ƒëƒÉng k√Ω xe m√°y (Motorcycle registration)
- C√† v·∫πt xe (Vehicle registration card)

JSON OUTPUT FORMAT:
{
  "vehicleType": "√î t√¥ | Xe m√°y | Xe t·∫£i | null",
  "licensePlate": "Bi·ªÉn s·ªë xe (VD: 30A-12345) | null",
  "chassisNumber": "S·ªë khung (VIN) | null",
  "engineNumber": "S·ªë m√°y | null",
  "brand": "H√£ng xe (Honda, Toyota, Yamaha...) | null",
  "model": "D√≤ng xe (SH Mode, Vios...) | null",
  "manufacturingYear": "NƒÉm s·∫£n xu·∫•t | null",
  "color": "M√†u s∆°n | null",
  "engineCapacity": "Dung t√≠ch xi lanh (cc) | null",
  "registrationDate": "Ng√†y ƒëƒÉng k√Ω DD/MM/YYYY | null",
  "ownerName": "T√™n ch·ªß xe | null",
  "ownerAddress": "ƒê·ªãa ch·ªâ ch·ªß xe | null",
  "documentType": "Vehicle Registration"
}

FIELD EXTRACTION RULES:

vehicleType:
- Extract from document type or vehicle classification
- Values: "√î t√¥", "Xe m√°y", "Xe t·∫£i", etc.

licensePlate:
- Extract from "Bi·ªÉn s·ªë ƒëƒÉng k√Ω" / "Bi·ªÉn ki·ªÉm so√°t"
- Format: XX[A-Z]-XXXXX (e.g., 30A-12345, 51H-98765)
- Keep dashes/spaces as shown

chassisNumber:
- Extract from "S·ªë khung" / "VIN"
- Usually 17-character alphanumeric code
- Keep exactly as shown

engineNumber:
- Extract from "S·ªë m√°y"
- Alphanumeric code
- Keep exactly as shown

brand:
- Extract from "Nh√£n hi·ªáu" / "H√£ng xe"
- Examples: Honda, Toyota, Yamaha, Suzuki, Hyundai

model:
- Extract from "Lo·∫°i xe" / "D√≤ng xe"  
- Examples: SH Mode, Wave Alpha, Vios, Accent

manufacturingYear:
- Extract from "NƒÉm s·∫£n xu·∫•t"
- 4-digit year: 2020, 2021, etc.

color:
- Extract from "M√†u s∆°n"
- Keep Vietnamese: ƒê·ªè, Xanh, Tr·∫Øng, ƒêen, etc.

engineCapacity:
- Extract from "Dung t√≠ch xi lanh"
- Number only (cc unit removed): 125, 150, 1500, etc.

registrationDate:
- Extract from "Ng√†y ƒëƒÉng k√Ω l·∫ßn ƒë·∫ßu"
- Format: DD/MM/YYYY

ownerName:
- Extract from "T√™n ch·ªß s·ªü h·ªØu"
- Keep Vietnamese characters

ownerAddress:
- Extract from "ƒê·ªãa ch·ªâ" of owner
- Full address if available

IMPORTANT:
- Return ONLY valid JSON (no markdown, no explanations)
- All text values must be properly escaped
- Use null (not "null" string) for missing fields
- Preserve Vietnamese diacritics exactly

Now extract vehicle information from this document:"""

# Configure Gemini API
GEMINI_API_KEY = "AIzaSyAVMe9ck7e7yX4F9__HIEkxUwq1XCSi4v0"
client = genai.Client(api_key=GEMINI_API_KEY)

# Insurance Chatbot Prompt - Smart advisor based on document analysis
INSURANCE_CHATBOT_PROMPT = """B·∫°n l√† AI T∆∞ v·∫•n vi√™n b·∫£o hi·ªÉm chuy√™n nghi·ªáp c·ªßa c√¥ng ty ADE Insurance.

üéØ NHI·ªÜM V·ª§:
- T∆∞ v·∫•n b·∫£o hi·ªÉm th√¥ng minh d·ª±a tr√™n ph√¢n t√≠ch t√†i li·ªáu kh√°ch h√†ng
- Gi·∫£i th√≠ch l·ª£i √≠ch & g·ª£i √Ω s·∫£n ph·∫©m ph√π h·ª£p theo v√πng mi·ªÅn
- Gi·ªçng ƒëi·ªáu chuy√™n nghi·ªáp, th√¢n thi·ªán, d·ªÖ hi·ªÉu

üîê QUY T·∫ÆC B·∫¢O M·∫¨T (CRITICAL):
‚ùå TUY·ªÜT ƒê·ªêI KH√îNG ƒë∆∞·ª£c ti·∫øt l·ªô:
  - S·ªë CMND/CCCD
  - ƒê·ªãa ch·ªâ chi ti·∫øt (ch·ªâ n√™u v√πng mi·ªÅn: B·∫Øc/Trung/Nam)
  - S·ªë ƒëi·ªán tho·∫°i
  - Email c√° nh√¢n
  - B·∫•t k·ª≥ th√¥ng tin nh·∫°y c·∫£m n√†o

‚úÖ CH·ªà ƒê∆Ø·ª¢C s·ª≠ d·ª•ng:
  - V√πng mi·ªÅn (B·∫Øc/Trung/Nam) ƒë·ªÉ g·ª£i √Ω
  - Lo·∫°i b·∫£o hi·ªÉm ph√π h·ª£p
  - Gi·∫£i th√≠ch quy·ªÅn l·ª£i
  - ∆Øu ƒë√£i & khuy·∫øn m√£i

üìã NGUY√äN T·∫ÆC TR·∫¢ L·ªúI:

1Ô∏è‚É£ NG·∫ÆN G·ªåN & R√ï R√ÄNG:
   - M·ªói c√¢u tr·∫£ l·ªùi 2-4 c√¢u
   - D√πng emoji ph√π h·ª£p (üè† üåä üöó ‚õàÔ∏è ‚úÖ)
   - Bullet points khi c·∫ßn li·ªát k√™

2Ô∏è‚É£ C√Å NH√ÇN H√ìA:
   - N·∫øu bi·∫øt v√πng mi·ªÅn ‚Üí g·ª£i √Ω b·∫£o hi·ªÉm thi√™n tai ph√π h·ª£p
   - Mi·ªÅn B·∫Øc: Ng·∫≠p l·ª•t m√πa m∆∞a
   - Mi·ªÅn Trung: B√£o & l≈© qu√©t
   - Mi·ªÅn Nam: Tri·ªÅu c∆∞·ªùng, ng·∫≠p √∫ng
   
3Ô∏è‚É£ T∆Ø V·∫§N TH√îNG MINH:
   - Gi·∫£i th√≠ch L√ù DO kh√°ch h√†ng n√™n mua
   - ƒê∆∞a ra 2-3 g√≥i ph√π h·ª£p nh·∫•t
   - K√™u g·ªçi h√†nh ƒë·ªông: "B·∫°n mu·ªën xem chi ti·∫øt kh√¥ng?"

4Ô∏è‚É£ X·ª¨ L√ù THI·∫æU TH√îNG TIN:
   - N·∫øu ch∆∞a c√≥ document ‚Üí khuy·∫øn kh√≠ch upload ƒë·ªÉ t∆∞ v·∫•n ch√≠nh x√°c
   - "T√¥i c·∫ßn ph√¢n t√≠ch h·ªì s∆° c·ªßa b·∫°n ƒë·ªÉ t∆∞ v·∫•n t·ªët h∆°n. B·∫°n c√≥ th·ªÉ upload CCCD kh√¥ng?"

5Ô∏è‚É£ UPSELL & CROSS-SELL:
   - G·ª£i √Ω combo: Nh√¢n th·ªç + S·ª©c kh·ªèe
   - ∆Øu ƒë√£i gia ƒë√¨nh
   - B·∫£o hi·ªÉm xe + Thi√™n tai

üìå C√ÅC C√ÇU H·ªéI TH∆Ø·ªúNG G·∫∂P:

Q: "T√¥i ·ªü mi·ªÅn Trung n√™n mua g√¨?"
A: "üåä Mi·ªÅn Trung ƒëang trong m√πa b√£o l≈©! 
G√≥i b·∫£o hi·ªÉm thi√™n tai s·∫Ω b·∫£o v·ªá nh√† c·ª≠a & ph∆∞∆°ng ti·ªán tr∆∞·ªõc ng·∫≠p l·ª•t.
‚úÖ Quy·ªÅn l·ª£i: ƒê·ªÅn b√π 100% gi√° tr·ªã khi thi·ªát h·∫°i
B·∫°n mu·ªën xem chi ti·∫øt g√≥i n√†o?"

Q: "Xe ng·∫≠p n∆∞·ªõc c√≥ b·ªìi th∆∞·ªùng kh√¥ng?"
A: "üöó C√ì! G√≥i b·∫£o hi·ªÉm thi√™n tai ph∆∞∆°ng ti·ªán b·ªìi th∆∞·ªùng:
‚úÖ Ng·∫≠p n∆∞·ªõc ƒë·ªông c∆°
‚úÖ H·ªèng h√≥c do m∆∞a l≈©
‚úÖ S·ª≠a ch·ªØa ho·∫∑c ƒë·ªÅn b√π 100%
Xe b·∫°n lo·∫°i n√†o ƒë·ªÉ t√¥i t∆∞ v·∫•n ch√≠nh x√°c?"

Q: "Gi·∫£i th√≠ch quy·ªÅn l·ª£i b·∫£o hi·ªÉm thi√™n tai"
A: "üè† B·∫£o hi·ªÉm thi√™n tai b·∫£o v·ªá:
‚úÖ Nh√† c·ª≠a: S·∫≠p ƒë·ªï, h∆∞ h·∫°i do b√£o
‚úÖ T√†i s·∫£n: ƒê·ªì d√πng, n·ªôi th·∫•t ng·∫≠p n∆∞·ªõc
‚úÖ Ph∆∞∆°ng ti·ªán: Xe m√°y, √¥ t√¥
üí∞ ƒê·ªÅn b√π l√™n ƒë·∫øn 500 tri·ªáu/s·ª± ki·ªán
B·∫°n mu·ªën mua g√≥i n√†o?"

üéØ TONE & STYLE:
- X∆∞ng h√¥: "B·∫°n" / "Anh/Ch·ªã" (tu·ª≥ ng·ªØ c·∫£nh)
- Th√¢n thi·ªán nh∆∞ng chuy√™n nghi·ªáp
- Tr√°nh thu·∫≠t ng·ªØ ph·ª©c t·∫°p
- Lu√¥n k·∫øt th√∫c b·∫±ng c√¢u h·ªèi m·ªü ƒë·ªÉ ti·∫øp t·ª•c t∆∞∆°ng t√°c

B√¢y gi·ªù h√£y tr·∫£ l·ªùi c√¢u h·ªèi sau c·ªßa kh√°ch h√†ng:"""

# Markdown Extraction Prompt - Optimized for large documents with tables
DOCUMENT_MARKDOWN_PROMPT = """You are an expert OCR and document analysis system specialized in extracting structured content from documents.

Your task is to extract ALL text content from this document image and format it as clean, well-structured Markdown.

CRITICAL RULES:
1. Extract EVERY piece of text visible in the document - do not skip any content
2. Maintain the original language - DO NOT translate
3. Preserve document structure with proper Markdown formatting
4. For TABLES: Use proper Markdown table syntax with aligned columns
5. For LISTS: Use appropriate list formatting (-, *, or numbered)
6. Maintain logical reading order (top to bottom, left to right)
7. Preserve all numbers, dates, codes, and special characters EXACTLY as shown
8. Keep paragraph breaks and spacing

OUTPUT REQUIREMENTS:
- Return ONLY Markdown text (no JSON, no explanations, no code blocks)
- Start directly with the document content
- Use proper Markdown syntax throughout

FORMATTING GUIDELINES:

Headers:
- Document title: # Title
- Major sections: ## Section Name  
- Subsections: ### Subsection Name
- Minor headings: #### Heading

Tables (CRITICAL for structured data - THIS IS THE MOST IMPORTANT):
| Column 1 | Column 2 | Column 3 |
|----------|----------|----------|
| Data 1   | Data 2   | Data 3   |
| Data 4   | Data 5   | Data 6   |

RULES FOR TABLES:
- ALWAYS detect tables in the document (forms, grids, structured data)
- MUST use proper Markdown table syntax with pipes |
- MUST include header row with column names
- MUST include separator row with dashes |----------|
- Align columns neatly with consistent spacing
- One row per data entry
- Preserve cell content EXACTLY as shown
- For empty cells, use empty space between pipes
- For merged cells, repeat content or use descriptive text
- Extract ALL rows visible in the table, not just sample rows

Example of properly formatted table:
| Species | Breed/Color | Age | Sex | Name |
|---------|-------------|-----|-----|------|
| Reindeer | Brown/White | Adult | M | DASHER |
| Reindeer | Brown/White | Adult | M | DANCER |
| Reindeer | Brown/White | Adult | M | PRANCER |

Lists:
- Unordered: - item or * item
- Ordered: 1. item, 2. item
- Nested: indent with 2 spaces

Text Formatting:
- **Bold** for important text
- *Italic* for emphasis
- `Code` for special values/codes
- > Quote for quoted sections

Preserve:
- Line breaks between paragraphs
- Spacing in formatted sections
- All punctuation and symbols
- Original text case

Now extract ALL content from the document, structure it logically, and format as Markdown:"""

# Insurance Recommendation Prompt - Region-based recommendations
INSURANCE_RECOMMENDATION_PROMPT = """üéØ ROLE:
B·∫°n l√† h·ªá th·ªëng "AI Insurance Recommendation Engine".
Nhi·ªám v·ª•: ƒë·ªçc t√†i li·ªáu (CCCD, gi·∫•y t·ªù ƒë·ªãnh danh, h·ª£p ƒë·ªìng‚Ä¶) v√† ch·ªâ c·∫ßn x√°c ƒë·ªãnh:
- ƒê·ªãa ch·ªâ th∆∞·ªùng tr√∫ ho·∫∑c t·∫°m tr√∫
- Qu√™ qu√°n (n∆°i sinh/nguy√™n qu√°n)
- Thu·ªôc mi·ªÅn B·∫Øc / mi·ªÅn Trung / mi·ªÅn Nam (Vi·ªát Nam)
Sau ƒë√≥ ƒë·ªÅ xu·∫•t c√°c g√≥i b·∫£o hi·ªÉm ph√π h·ª£p v·ªõi r·ªßi ro v√πng mi·ªÅn.

---

üìå INPUT:
1 h√¨nh ·∫£nh ho·∫∑c t√†i li·ªáu c√≥ ch·ª©a ƒë·ªãa ch·ªâ c∆∞ tr√∫/t·∫°m tr√∫ v√†/ho·∫∑c qu√™ qu√°n b·∫±ng ti·∫øng Vi·ªát.

---

üìå OUTPUT ‚Äî TR·∫¢ V·ªÄ JSON H·ª¢P L·ªÜ DUY NH·∫§T:

{
  "address": {
      "text": "...",
      "type": "thuong_tru" | "tam_tru" | "unknown",
      "region": "Bac" | "Trung" | "Nam" | "Unknown"
  },
  "place_of_origin": {
      "text": "...",
      "region": "Bac" | "Trung" | "Nam" | "Unknown"
  },
  "recommended_packages": [
      {
        "name": "...",
        "reason": "...",
        "priority": 0.0-1.0
      }
  ]
}

---

üìå LOGIC G·ª¢I √ù G√ìI B·∫¢O HI·ªÇM:

**QUY T·∫ÆC ∆ØU TI√äN:**
1. Ph√¢n t√≠ch qu√™ qu√°n tr∆∞·ªõc (place_of_origin)
2. N·∫øu qu√™ qu√°n l√† B·∫Øc ho·∫∑c Trung ‚Üí ƒê·ªÄ XU·∫§T NGAY, kh√¥ng c·∫ßn ki·ªÉm tra ƒë·ªãa ch·ªâ th∆∞·ªùng tr√∫
3. N·∫øu qu√™ qu√°n l√† Nam ‚Üí ki·ªÉm tra ƒë·ªãa ch·ªâ th∆∞·ªùng tr√∫ (address)
   - N·∫øu ƒë·ªãa ch·ªâ th∆∞·ªùng tr√∫ l√† B·∫Øc/Trung ‚Üí ƒê·ªÄ XU·∫§T
   - N·∫øu ƒë·ªãa ch·ªâ th∆∞·ªùng tr√∫ c≈©ng l√† Nam ‚Üí KH√îNG ƒê·ªÄ XU·∫§T
4. N·∫øu kh√¥ng c√≥ qu√™ qu√°n ‚Üí d√πng ƒë·ªãa ch·ªâ th∆∞·ªùng tr√∫

**ƒêI·ªÄU KI·ªÜN ƒê·ªÄ XU·∫§T:**

N·∫øu (place_of_origin.region == "Bac" ho·∫∑c "Trung") HO·∫∂C (place_of_origin.region == "Nam" V√Ä address.region == "Bac" ho·∫∑c "Trung"):
  - Add:
  - Add:
      1Ô∏è‚É£ B·∫£o hi·ªÉm thi√™n tai ng·∫≠p l·ª•t
         - priority: 0.95
         - reason: "Khu v·ª±c mi·ªÅn [B·∫Øc/Trung] th∆∞·ªùng xuy√™n ch·ªãu ·∫£nh h∆∞·ªüng b·ªüi b√£o v√† m∆∞a l≈©. G√≥i b·∫£o hi·ªÉm n√†y b·∫£o v·ªá t√†i s·∫£n kh·ªèi thi·ªát h·∫°i do ng·∫≠p l·ª•t, l≈© qu√©t."
      
      2Ô∏è‚É£ B·∫£o hi·ªÉm nh√† c·ª≠a tr∆∞·ªõc b√£o
         - priority: 0.90
         - reason: "B√£o v√† gi√≥ m·∫°nh th∆∞·ªùng x·∫£y ra t·∫°i mi·ªÅn [B·∫Øc/Trung], g√¢y h∆∞ h·∫°i cho m√°i nh√†, c·ª≠a s·ªï, t∆∞·ªùng. G√≥i n√†y ƒë·∫£m b·∫£o chi ph√≠ s·ª≠a ch·ªØa ho·∫∑c x√¢y d·ª±ng l·∫°i."
      
      3Ô∏è‚É£ B·∫£o hi·ªÉm ph∆∞∆°ng ti·ªán ng·∫≠p n∆∞·ªõc
         - priority: 0.85
         - reason: "Xe m√°y, √¥ t√¥ d·ªÖ b·ªã ng·∫≠p n∆∞·ªõc khi m∆∞a l·ªõn ho·∫∑c l≈© l·ª•t. G√≥i n√†y gi√∫p b·ªìi th∆∞·ªùng chi ph√≠ s·ª≠a ch·ªØa ƒë·ªông c∆°, h·ªá th·ªëng ƒëi·ªán b·ªã h∆∞ h·ªèng do n∆∞·ªõc."

N·∫øu (place_of_origin.region == "Nam" V√Ä address.region == "Nam") HO·∫∂C (c·∫£ 2 ƒë·ªÅu Unknown):
  - Kh√¥ng ƒë·ªÅ xu·∫•t g√¨ (ƒë·ªÉ m·∫£ng recommended_packages r·ªóng: [])
  - Gi·ªØ ƒë·∫ßy ƒë·ªß key theo JSON format

**V√ç D·ª§ MINH H·ªåA:**
- Qu√™ qu√°n: H√† Tƒ©nh (Trung) ‚Üí ƒê·ªÄ XU·∫§T 3 g√≥i (b·∫•t k·ªÉ ƒë·ªãa ch·ªâ th∆∞·ªùng tr√∫ ·ªü ƒë√¢u)
- Qu√™ qu√°n: TP.HCM (Nam), ƒê·ªãa ch·ªâ: H√† N·ªôi (B·∫Øc) ‚Üí ƒê·ªÄ XU·∫§T 3 g√≥i
- Qu√™ qu√°n: TP.HCM (Nam), ƒê·ªãa ch·ªâ: C·∫ßn Th∆° (Nam) ‚Üí KH√îNG ƒë·ªÅ xu·∫•t
- Qu√™ qu√°n: Unknown, ƒê·ªãa ch·ªâ: Ngh·ªá An (Trung) ‚Üí ƒê·ªÄ XU·∫§T 3 g√≥i

---

üìå X√ÅC ƒê·ªäNH V√ôNG MI·ªÄN:

**C√ÅCH NH·∫¨N BI·∫æT QU√ä QU√ÅN TR√äN CCCD/CMND:**
- T√¨m d√≤ng c√≥ ch·ªØ: "Qu√™ qu√°n" | "Place of origin" | "Nguy√™n qu√°n"
- Th∆∞·ªùng n·∫±m ·ªü m·∫∑t SAU c·ªßa CCCD (CCCD m·ªõi g·∫Øn chip)
- Ho·∫∑c ·ªü m·∫∑t TR∆Ø·ªöC c·ªßa CMND (CMND c≈© 9 s·ªë)
- Format: "Qu√™ qu√°n: [X√£/Ph∆∞·ªùng], [Huy·ªán/Qu·∫≠n], [T·ªânh/Th√†nh ph·ªë]"
- V√≠ d·ª•: "Qu√™ qu√°n: X√£ H√≤a B√¨nh, Huy·ªán T√¢n L·∫°c, H√≤a B√¨nh"
- Ch·ªâ c·∫ßn t·ªânh/th√†nh ph·ªë cu·ªëi c√πng ƒë·ªÉ x√°c ƒë·ªãnh v√πng mi·ªÅn

**PH√ÇN LO·∫†I MI·ªÄN:**

MI·ªÄN B·∫ÆC (Bac):
- H√† N·ªôi, H·∫£i Ph√≤ng, Qu·∫£ng Ninh, H·∫£i D∆∞∆°ng, H∆∞ng Y√™n, B·∫Øc Ninh, Vƒ©nh Ph√∫c, Ph√∫ Th·ªç
- Th√°i Nguy√™n, B·∫Øc Giang, L·∫°ng S∆°n, Cao B·∫±ng, L√†o Cai, Y√™n B√°i, Tuy√™n Quang
- H√≤a B√¨nh, S∆°n La, Lai Ch√¢u, ƒêi·ªán Bi√™n, H√† Giang
- Ninh B√¨nh, Nam ƒê·ªãnh, Th√°i B√¨nh

MI·ªÄN TRUNG (Trung):
- Thanh H√≥a, Ngh·ªá An, H√† Tƒ©nh, Qu·∫£ng B√¨nh, Qu·∫£ng Tr·ªã, Th·ª´a Thi√™n Hu·∫ø
- ƒê√† N·∫µng, Qu·∫£ng Nam, Qu·∫£ng Ng√£i, B√¨nh ƒê·ªãnh
- Ph√∫ Y√™n, Kh√°nh H√≤a, Ninh Thu·∫≠n, B√¨nh Thu·∫≠n
- Kon Tum, Gia Lai, ƒê·∫Øk L·∫Øk, ƒê·∫Øk N√¥ng, L√¢m ƒê·ªìng

MI·ªÄN NAM (Nam):
- TP. H·ªì Ch√≠ Minh (TP.HCM, S√†i G√≤n)
- B√† R·ªãa - V≈©ng T√†u, ƒê·ªìng Nai, B√¨nh D∆∞∆°ng, B√¨nh Ph∆∞·ªõc, T√¢y Ninh
- Long An, Ti·ªÅn Giang, B·∫øn Tre, Tr√† Vinh, Vƒ©nh Long
- ƒê·ªìng Th√°p, An Giang, Ki√™n Giang, C·∫ßn Th∆°, H·∫≠u Giang
- S√≥c TrƒÉng, B·∫°c Li√™u, C√† Mau

---

üìå Y√äU C·∫¶U B·∫ÆT BU·ªòC:
- Kh√¥ng tr·∫£ l·ªùi g√¨ ngo√†i JSON
- JSON ph·∫£i h·ª£p l·ªá tuy·ªát ƒë·ªëi
- N·∫øu thi·∫øu d·ªØ li·ªáu ‚Üí v·∫´n gi·ªØ key & g√°n gi√° tr·ªã "Unknown" ho·∫∑c []
- Kh√¥ng l∆∞u l·∫°i hay m√¥ t·∫£ n·ªôi dung h√¨nh ·∫£nh
- Tr√≠ch xu·∫•t ƒë·ªãa ch·ªâ v√† qu√™ qu√°n CH√çNH X√ÅC nh∆∞ trong t√†i li·ªáu (gi·ªØ nguy√™n ti·∫øng Vi·ªát c√≥ d·∫•u)
- **LU√îN tr√≠ch xu·∫•t C·∫¢ HAI:** qu√™ qu√°n (place_of_origin) v√† ƒë·ªãa ch·ªâ th∆∞·ªùng tr√∫ (address)
- **∆ØU TI√äN qu√™ qu√°n** ƒë·ªÉ ƒë·ªÅ xu·∫•t, nh∆∞ng v·∫´n c·∫ßn c·∫£ 2 th√¥ng tin ƒë·ªÉ ƒë∆∞a ra quy·∫øt ƒë·ªãnh ch√≠nh x√°c
- X√°c ƒë·ªãnh v√πng mi·ªÅn cho C·∫¢ qu√™ qu√°n V√Ä ƒë·ªãa ch·ªâ th∆∞·ªùng tr√∫
- √Åp d·ª•ng ƒë√∫ng logic ƒë·ªÅ xu·∫•t theo quy t·∫Øc ∆∞u ti√™n ·ªü tr√™n

B√¢y gi·ªù ph√¢n t√≠ch t√†i li·ªáu v√† tr·∫£ v·ªÅ JSON:"""

# Document Analysis Prompt - Enhanced for better JSON structure
DOCUMENT_AUTO_ANALYSIS_PROMPT = """You are an expert document analyzer for insurance and legal documents.

Your task is to analyze this document image and extract structured information in valid JSON format.

CRITICAL RULES:
1. Automatically detect the document type (e.g., "Insurance Claim Form", "Policy Document", "Contract", "Invoice", "Medical Report", "ID Card", "Veterinary Certificate", etc.)
2. Extract ONLY information that is ACTUALLY PRESENT and CLEARLY VISIBLE in the document
3. DO NOT invent, guess, or infer information not explicitly shown
4. Support ALL languages: Keep original language - DO NOT translate
5. For tables/structured data: Extract each row as a separate entry with clear field:value pairs
6. For dates: Extract ONLY explicitly written dates (format: YYYY-MM-DD or preserve original format)
7. Detect signatures, stamps, seals, checkmarks, or handwritten annotations

SPECIAL HANDLING FOR TABLES:
- If document contains tables (forms, grids), extract EACH ROW as a separate "number" entry
- Format table data as clear field-value pairs
- Example: {"label": "Animal 1 - Reindeer DASHER", "value": "Species: Reindeer, Name: DASHER, Sex: M, Age: Adult"}
- Extract ALL visible rows, not just samples
- Preserve column headers as field names

OUTPUT FORMAT:
- Return ONLY valid JSON (no markdown, no explanations, no code blocks)
- Use null for missing text fields, [] for missing arrays, false for booleans
- Ensure all strings are properly escaped
- All values must be extracted from the document, not inferred

JSON SCHEMA:
{
  "document_type": "specific type of document",
  "confidence": 0.0-1.0,
  "title": "document title if present | null",
  "summary": "concise 2-3 sentence summary of key information",
  "people": [
    {"name": "Full Name", "role": "Insured | Claimant | Witness | Doctor | etc. | null"}
  ],
  "organizations": [
    {"name": "Company/Organization Name"}
  ],
  "locations": [
    {"name": "Full Address or Location"}
  ],
  "dates": [
    {"label": "Date of Birth | Effective Date | Claim Date | etc.", "value": "YYYY-MM-DD"}
  ],
  "numbers": [
    {"label": "Policy Number | Claim Number | Amount | Phone | ID | Account | etc.", "value": "exact value as string"}
  ],
  "signature_detected": true | false
}

EXTRACTION GUIDELINES:

People: 
- Names of individuals mentioned with their role
- Examples: policy holder, insured person, claimant, beneficiary, witness, doctor, agent

Organizations:
- Insurance companies, hospitals, clinics, employers, service providers
- Extract full official names

Locations:
- Complete addresses (street, city, state, postal code)
- Separate entries for different locations

Dates:
- ONLY dates explicitly written in the document
- Common types: birth date, issue date, effective date, expiry date, claim date, incident date
- Format as YYYY-MM-DD (convert from any format shown)

Numbers (CRITICAL for tables):
- Policy/Certificate numbers
- Claim/Case numbers  
- Monetary amounts (with currency if shown)
- Phone numbers, fax numbers
- ID numbers, license numbers
- Account numbers
- Percentages, quantities
- Each table row's key data as separate entries

Signatures:
- true if handwritten signature, stamp, seal, or official mark is visible
- false if no signature visible

IMPORTANT:
- If document has tables: Extract each important row's data as separate number entries
- Preserve all numbers exactly (including leading zeros, dashes, spaces)
- For empty fields: use null or []
- Extract what you SEE, not what you think should be there

Now analyze the document and return ONLY the JSON object:"""


def clean_json_response(response_text: str) -> str:
    """
    Clean JSON response by removing markdown wrappers and extra text
    """
    # Remove markdown code blocks
    text = re.sub(r'```json\s*', '', response_text)
    text = re.sub(r'```\s*', '', text)
    
    # Remove any text before first { and after last }
    text = text.strip()
    
    # Find first { and last }
    start_idx = text.find('{')
    end_idx = text.rfind('}')
    
    if start_idx != -1 and end_idx != -1:
        text = text[start_idx:end_idx+1]
    
    return text.strip()


def validate_json_schema(data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Validate and ensure JSON follows the required schema
    """
    # Ensure all required fields exist
    schema = {
        "document_type": data.get("document_type", "Unknown Document"),
        "confidence": float(data.get("confidence", 0.0)),
        "title": data.get("title"),
        "summary": data.get("summary", ""),
        "people": data.get("people", []),
        "organizations": data.get("organizations", []),
        "locations": data.get("locations", []),
        "dates": data.get("dates", []),
        "numbers": data.get("numbers", []),
        "signature_detected": bool(data.get("signature_detected", False))
    }
    
    # Ensure confidence is between 0 and 1
    if schema["confidence"] < 0:
        schema["confidence"] = 0.0
    elif schema["confidence"] > 1:
        schema["confidence"] = 1.0
    
    return schema


async def analyze_auto_document(image_path: str) -> Dict[str, Any]:
    """
    Analyze document using Gemini 2.5 Flash
    Optimized for large files with automatic image optimization
    
    Args:
        image_path: Path to the image file
        
    Returns:
        Dictionary containing structured analysis results
    """
    try:
        # Load image
        if not os.path.exists(image_path):
            return {
                "error": f"Image file not found: {image_path}",
                "document_type": "Error",
                "confidence": 0.0,
                "title": None,
                "summary": "Failed to load document image",
                "people": [],
                "organizations": [],
                "locations": [],
                "dates": [],
                "numbers": [],
                "signature_detected": False
            }
        
        # Open image with PIL
        image = Image.open(image_path)
        
        # Convert to RGB if necessary
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Optimize image for large files (> 2MB or > 4000px)
        file_size = os.path.getsize(image_path) / (1024 * 1024)  # Size in MB
        max_dimension = max(image.size)
        
        if file_size > 2 or max_dimension > 4000:
            # Calculate new size while maintaining aspect ratio
            max_size = 3000  # Maximum dimension
            if max_dimension > max_size:
                ratio = max_size / max_dimension
                new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))
                image = image.resize(new_size, Image.Resampling.LANCZOS)
                print(f"   üìê Optimized image from {image_path} to {new_size} (original: {max_dimension}px, {file_size:.1f}MB)")
        
        # Convert PIL Image to bytes
        img_byte_arr = io.BytesIO()
        image.save(img_byte_arr, format='JPEG')
        img_byte_arr = img_byte_arr.getvalue()
        
        # Generate content with Gemini 2.0 Flash
        response = client.models.generate_content(
            model='gemini-2.5-flash',
            contents=[
                types.Content(
                    role='user',
                    parts=[
                        types.Part(text=DOCUMENT_AUTO_ANALYSIS_PROMPT),
                        types.Part(
                            inline_data=types.Blob(
                                data=img_byte_arr,
                                mime_type='image/jpeg'
                            )
                        )
                    ]
                )
            ],
            config=types.GenerateContentConfig(
                temperature=0.1,
                top_p=0.95,
                top_k=40,
                max_output_tokens=8192
            )
        )
        
        # Get response text
        response_text = response.text
        
        # Clean JSON response
        cleaned_json = clean_json_response(response_text)
        
        # Parse JSON
        try:
            result = json.loads(cleaned_json)
            # Validate schema
            result = validate_json_schema(result)
            return result
        except json.JSONDecodeError as e:
            # If JSON parsing fails, return error with raw response
            print(f"JSON Parse Error: {e}")
            print(f"Raw response: {response_text}")
            print(f"Cleaned JSON: {cleaned_json}")
            
            return {
                "error": f"Failed to parse JSON response: {str(e)}",
                "raw_response": response_text[:500],  # First 500 chars for debugging
                "document_type": "Parse Error",
                "confidence": 0.0,
                "title": None,
                "summary": "Failed to parse AI response",
                "people": [],
                "organizations": [],
                "locations": [],
                "dates": [],
                "numbers": [],
                "signature_detected": False
            }
            
    except Exception as e:
        # Handle any other errors
        print(f"Error in analyze_auto_document: {e}")
        return {
            "error": str(e),
            "document_type": "Error",
            "confidence": 0.0,
            "title": None,
            "summary": f"Analysis failed: {str(e)}",
            "people": [],
            "organizations": [],
            "locations": [],
            "dates": [],
            "numbers": [],
            "signature_detected": False
        }


async def extract_markdown_content(image_path: str) -> str:
    """
    Extract full text content from document as Markdown
    Optimized for large files with automatic image optimization
    
    Args:
        image_path: Path to the image file
        
    Returns:
        Markdown formatted text content
    """
    try:
        # Load image
        if not os.path.exists(image_path):
            return f"# Error\n\nImage file not found: {image_path}"
        
        # Open image with PIL
        image = Image.open(image_path)
        
        # Convert to RGB if necessary
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Optimize image for large files (> 2MB or > 4000px)
        file_size = os.path.getsize(image_path) / (1024 * 1024)  # Size in MB
        max_dimension = max(image.size)
        
        if file_size > 2 or max_dimension > 4000:
            # Calculate new size while maintaining aspect ratio
            max_size = 3000  # Maximum dimension
            if max_dimension > max_size:
                ratio = max_size / max_dimension
                new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))
                image = image.resize(new_size, Image.Resampling.LANCZOS)
                print(f"   üìê Optimized image from {image_path} to {new_size} (original: {max_dimension}px, {file_size:.1f}MB)")
        
        # Convert PIL Image to bytes
        img_byte_arr = io.BytesIO()
        image.save(img_byte_arr, format='JPEG')
        img_byte_arr = img_byte_arr.getvalue()
        
        # Generate content with Gemini 2.0 Flash
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp',
            contents=[
                types.Content(
                    role='user',
                    parts=[
                        types.Part(text=DOCUMENT_MARKDOWN_PROMPT),
                        types.Part(
                            inline_data=types.Blob(
                                data=img_byte_arr,
                                mime_type='image/jpeg'
                            )
                        )
                    ]
                )
            ],
            config=types.GenerateContentConfig(
                temperature=0.1,
                top_p=0.95,
                top_k=40,
                max_output_tokens=8192
            )
        )
        
        # Get response text
        markdown_text = response.text
        
        # Clean up any markdown code blocks if present
        markdown_text = re.sub(r'^```markdown\s*', '', markdown_text, flags=re.MULTILINE)
        markdown_text = re.sub(r'^```\s*$', '', markdown_text, flags=re.MULTILINE)
        markdown_text = markdown_text.strip()
        
        return markdown_text
        
    except Exception as e:
        # Handle any errors
        print(f"Error in extract_markdown_content: {e}")
        import traceback
        traceback.print_exc()
        return f"# Error\n\nFailed to extract text: {str(e)}"


def get_image_path_from_url(image_url: str) -> Optional[str]:
    """
    Convert image URL to local file path
    
    Args:
        image_url: URL path like "/data/images/xxx.png"
        
    Returns:
        Local file path or None if invalid
    """
    if not image_url:
        return None
    
    # Remove leading slash and /data/ prefix
    if image_url.startswith('/data/'):
        path = image_url[6:]  # Remove '/data/'
        return f"data/{path}"
    elif image_url.startswith('/'):
        path = image_url[1:]  # Remove leading '/'
        return path
    
    return image_url


async def extract_person_info(image_path: str) -> Dict[str, Any]:
    """
    Extract personal information from CCCD/ID/Driver License using Gemini
    
    Args:
        image_path: Path to the image file
        
    Returns:
        Dictionary containing personal information
    """
    try:
        # Load image
        if not os.path.exists(image_path):
            return {
                "error": f"Image file not found: {image_path}",
                "fullName": None,
                "dateOfBirth": None,
                "gender": None,
                "idNumber": None,
                "address": None,
                "phone": None,
                "email": None,
                "placeOfOrigin": None,
                "nationality": None,
                "issueDate": None,
                "expiryDate": None,
                "documentType": None
            }
        
        # Open image with PIL
        image = Image.open(image_path)
        
        # Convert to RGB if necessary
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Optimize image
        file_size = os.path.getsize(image_path) / (1024 * 1024)
        max_dimension = max(image.size)
        
        if file_size > 2 or max_dimension > 4000:
            max_size = 3000
            if max_dimension > max_size:
                ratio = max_size / max_dimension
                new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))
                image = image.resize(new_size, Image.Resampling.LANCZOS)
                print(f"   üìê Optimized image to {new_size}")
        
        # Convert to bytes
        img_byte_arr = io.BytesIO()
        image.save(img_byte_arr, format='JPEG')
        img_byte_arr = img_byte_arr.getvalue()
        
        # Call Gemini API with retry logic for quota errors
        max_retries = 3
        retry_delay = 2  # seconds
        response = None
        
        for attempt in range(max_retries):
            try:
                response = client.models.generate_content(
                    model='gemini-2.0-flash-exp',
                    contents=[
                        types.Content(
                            role='user',
                            parts=[
                                types.Part(text=PERSON_INFO_EXTRACTION_PROMPT),
                                types.Part(
                                    inline_data=types.Blob(
                                        data=img_byte_arr,
                                        mime_type='image/jpeg'
                                    )
                                )
                            ]
                        )
                    ],
                    config=types.GenerateContentConfig(
                        temperature=0.1,
                        top_p=0.95,
                        top_k=40,
                        max_output_tokens=2048
                    )
                )
                break  # Success, exit retry loop
                
            except Exception as api_error:
                error_msg = str(api_error)
                
                # Check if it's a quota error
                if "429" in error_msg or "RESOURCE_EXHAUSTED" in error_msg or "quota" in error_msg.lower():
                    print(f"   ‚ö†Ô∏è  Quota exceeded (attempt {attempt + 1}/{max_retries})")
                    
                    if attempt < max_retries - 1:
                        import time
                        wait_time = retry_delay * (attempt + 1)
                        print(f"   ‚è≥ Waiting {wait_time}s before retry...")
                        time.sleep(wait_time)
                        continue
                    else:
                        # All retries exhausted, return fallback data
                        print(f"   ‚ö†Ô∏è  API quota exhausted. Returning empty data for manual entry...")
                        return {
                            "fullName": None,
                            "dateOfBirth": None,
                            "gender": None,
                            "idNumber": None,
                            "address": None,
                            "phone": None,
                            "email": None,
                            "placeOfOrigin": None,
                            "nationality": "Vi·ªát Nam",
                            "issueDate": None,
                            "expiryDate": None,
                            "documentType": "CCCD",
                            "extractionStatus": "quota_exceeded",
                            "message": "‚ö†Ô∏è API quota ƒë√£ h·∫øt (50 requests/ng√†y). Vui l√≤ng nh·∫≠p th√¥ng tin th·ªß c√¥ng ho·∫∑c th·ª≠ l·∫°i sau 24h."
                        }
                else:
                    # Other API errors, re-raise
                    raise api_error
        
        # If response is None after retries, return error
        if response is None:
            return {
                "fullName": None,
                "dateOfBirth": None,
                "gender": None,
                "idNumber": None,
                "address": None,
                "phone": None,
                "email": None,
                "placeOfOrigin": None,
                "nationality": None,
                "issueDate": None,
                "expiryDate": None,
                "documentType": None,
                "extractionStatus": "failed",
                "message": "‚ùå Extraction failed after retries"
            }
        
        # Get response
        response_text = response.text
        
        # Clean JSON
        cleaned_json = clean_json_response(response_text)
        
        # Parse JSON
        try:
            result = json.loads(cleaned_json)
            print(f"‚úÖ Extracted person info: {result.get('fullName', 'N/A')}")
            return result
        except json.JSONDecodeError as e:
            print(f"‚ùå JSON Parse Error: {e}")
            print(f"Raw response: {response_text}")
            
            return {
                "error": f"Failed to parse JSON: {str(e)}",
                "raw_response": response_text[:500],
                "fullName": None,
                "dateOfBirth": None,
                "gender": None,
                "idNumber": None,
                "address": None,
                "phone": None,
                "email": None,
                "placeOfOrigin": None,
                "nationality": None,
                "issueDate": None,
                "expiryDate": None,
                "documentType": None
            }
            
    except Exception as e:
        print(f"‚ùå Error in extract_person_info: {e}")
        import traceback
        traceback.print_exc()
        return {
            "error": str(e),
            "fullName": None,
            "dateOfBirth": None,
            "gender": None,
            "idNumber": None,
            "address": None,
            "phone": None,
            "email": None,
            "placeOfOrigin": None,
            "nationality": None,
            "issueDate": None,
            "expiryDate": None,
            "documentType": None
        }


async def extract_vehicle_info(image_path: str) -> Dict[str, Any]:
    """
    Extract vehicle information from Vehicle Registration (C√† v·∫πt) using Gemini
    
    Args:
        image_path: Path to the vehicle registration image file
        
    Returns:
        Dictionary containing vehicle information
    """
    try:
        # Load image
        if not os.path.exists(image_path):
            return {
                "error": f"Image file not found: {image_path}",
                "vehicleType": None,
                "licensePlate": None,
                "chassisNumber": None,
                "engineNumber": None,
                "brand": None,
                "model": None,
                "manufacturingYear": None,
                "color": None,
                "engineCapacity": None,
                "registrationDate": None,
                "ownerName": None,
                "ownerAddress": None,
                "documentType": "Vehicle Registration"
            }
        
        # Open image with PIL
        image = Image.open(image_path)
        
        # Convert to RGB if necessary
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Optimize image
        file_size = os.path.getsize(image_path) / (1024 * 1024)
        max_dimension = max(image.size)
        
        if file_size > 2 or max_dimension > 4000:
            max_size = 3000
            if max_dimension > max_size:
                ratio = max_size / max_dimension
                new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))
                image = image.resize(new_size, Image.Resampling.LANCZOS)
                print(f"   üìê Optimized image to {new_size}")
        
        # Convert to bytes
        img_byte_arr = io.BytesIO()
        image.save(img_byte_arr, format='JPEG')
        img_byte_arr = img_byte_arr.getvalue()
        
        # Call Gemini API
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp',
            contents=[
                types.Content(
                    role='user',
                    parts=[
                        types.Part(text=VEHICLE_INFO_EXTRACTION_PROMPT),
                        types.Part(
                            inline_data=types.Blob(
                                data=img_byte_arr,
                                mime_type='image/jpeg'
                            )
                        )
                    ]
                )
            ],
            config=types.GenerateContentConfig(
                temperature=0.1,
                top_p=0.95,
                top_k=40,
                max_output_tokens=2048
            )
        )
        
        # Extract JSON from response
        response_text = response.text.strip()
        
        # Remove markdown code blocks if present
        if response_text.startswith('```'):
            response_text = re.sub(r'^```(?:json)?\n', '', response_text)
            response_text = re.sub(r'\n```$', '', response_text)
        
        # Parse JSON
        try:
            vehicle_data = json.loads(response_text)
            print(f"   ‚úÖ Vehicle info extracted: {vehicle_data.get('licensePlate', 'N/A')}")
            return vehicle_data
        except json.JSONDecodeError as json_err:
            print(f"   ‚ö†Ô∏è  JSON parse error: {json_err}")
            print(f"   üìÑ Raw response: {response_text[:500]}")
            return {
                "error": f"Invalid JSON response: {json_err}",
                "raw_response": response_text,
                "vehicleType": None,
                "licensePlate": None,
                "chassisNumber": None,
                "engineNumber": None,
                "brand": None,
                "model": None,
                "manufacturingYear": None,
                "color": None,
                "engineCapacity": None,
                "registrationDate": None,
                "ownerName": None,
                "ownerAddress": None,
                "documentType": "Vehicle Registration"
            }
            
    except Exception as e:
        print(f"‚ùå Error in extract_vehicle_info: {e}")
        import traceback
        traceback.print_exc()
        return {
            "error": str(e),
            "vehicleType": None,
            "licensePlate": None,
            "chassisNumber": None,
            "engineNumber": None,
            "brand": None,
            "model": None,
            "manufacturingYear": None,
            "color": None,
            "engineCapacity": None,
            "registrationDate": None,
            "ownerName": None,
            "ownerAddress": None,
            "documentType": "Vehicle Registration"
        }


async def recommend_insurance_by_address(image_path: str) -> Dict[str, Any]:
    """
    Analyze document address and recommend insurance packages based on region
    
    Args:
        image_path: Path to the image file (CCCD, ID, contract, etc.)
        
    Returns:
        Dictionary containing address analysis and insurance recommendations
        {
            "address": {
                "text": "Full address",
                "type": "thuong_tru | tam_tru | unknown",
                "region": "Bac | Trung | Nam | Unknown"
            },
            "place_of_origin": {
                "text": "Place of origin",
                "region": "Bac | Trung | Nam | Unknown"
            },
            "recommended_packages": [
                {
                    "name": "Package name",
                    "reason": "Reason for recommendation",
                    "priority": 0.0-1.0
                }
            ]
        }
    """
    try:
        # Load image
        if not os.path.exists(image_path):
            return {
                "error": f"Image file not found: {image_path}",
                "address": {
                    "text": "",
                    "type": "unknown",
                    "region": "Unknown"
                },
                "place_of_origin": {
                    "text": "",
                    "region": "Unknown"
                },
                "recommended_packages": []
            }
        
        # Open image with PIL
        image = Image.open(image_path)
        
        # Convert to RGB if necessary
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Optimize image
        file_size = os.path.getsize(image_path) / (1024 * 1024)
        max_dimension = max(image.size)
        
        if file_size > 2 or max_dimension > 4000:
            max_size = 3000
            if max_dimension > max_size:
                ratio = max_size / max_dimension
                new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))
                image = image.resize(new_size, Image.Resampling.LANCZOS)
                print(f"   üìê Optimized image to {new_size}")
        
        # Convert to bytes
        img_byte_arr = io.BytesIO()
        image.save(img_byte_arr, format='JPEG')
        img_byte_arr = img_byte_arr.getvalue()
        
        # Call Gemini API
        response = client.models.generate_content(
            model='gemini-2.0-flash-exp',
            contents=[
                types.Content(
                    role='user',
                    parts=[
                        types.Part(text=INSURANCE_RECOMMENDATION_PROMPT),
                        types.Part(
                            inline_data=types.Blob(
                                data=img_byte_arr,
                                mime_type='image/jpeg'
                            )
                        )
                    ]
                )
            ],
            config=types.GenerateContentConfig(
                temperature=0.1,
                top_p=0.95,
                top_k=40,
                max_output_tokens=2048
            )
        )
        
        # Get response
        response_text = response.text
        
        # Clean JSON
        cleaned_json = clean_json_response(response_text)
        
        # Parse JSON
        try:
            result = json.loads(cleaned_json)
            place_region = result.get('place_of_origin', {}).get('region', 'Unknown')
            addr_region = result.get('address', {}).get('region', 'Unknown')
            print(f"‚úÖ Qu√™ qu√°n: {place_region}, Address: {addr_region}")
            print(f"   üì¶ {len(result.get('recommended_packages', []))} packages recommended")
            return result
        except json.JSONDecodeError as e:
            print(f"‚ùå JSON Parse Error: {e}")
            print(f"Raw response: {response_text}")
            
            return {
                "error": f"Failed to parse JSON: {str(e)}",
                "raw_response": response_text[:500],
                "address": {
                    "text": "",
                    "type": "unknown",
                    "region": "Unknown"
                },
                "place_of_origin": {
                    "text": "",
                    "region": "Unknown"
                },
                "recommended_packages": []
            }
            
    except Exception as e:
        print(f"‚ùå Error in recommend_insurance_by_address: {e}")
        import traceback
        traceback.print_exc()
        return {
            "error": str(e),
            "address": {
                "text": "",
                "type": "unknown",
                "region": "Unknown"
            },
            "place_of_origin": {
                "text": "",
                "region": "Unknown"
            },
            "recommended_packages": []
        }


async def recommend_insurance_by_person_info(person_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Recommend insurance packages based on extracted PersonInfo data
    Uses placeOfOrigin and address from already extracted data
    
    Args:
        person_data: PersonInfo dict with placeOfOrigin, address, etc.
        
    Returns:
        Dictionary containing insurance recommendations based on region
    """
    try:
        place_of_origin = person_data.get('placeOfOrigin', '')
        address = person_data.get('address', '')
        
        print(f"   üè† Analyzing: placeOfOrigin='{place_of_origin}', address='{address}'")
        
        # Helper function to determine region from location text
        def get_region(location_text: str) -> str:
            if not location_text:
                return "Unknown"
            
            location_lower = location_text.lower()
            
            # Mi·ªÅn B·∫Øc
            bac_provinces = [
                'h√† n·ªôi', 'h·∫£i ph√≤ng', 'qu·∫£ng ninh', 'h·∫£i d∆∞∆°ng', 'h∆∞ng y√™n', 'b·∫Øc ninh', 
                'vƒ©nh ph√∫c', 'ph√∫ th·ªç', 'th√°i nguy√™n', 'b·∫Øc giang', 'l·∫°ng s∆°n', 'cao b·∫±ng',
                'l√†o cai', 'y√™n b√°i', 'tuy√™n quang', 'h√≤a b√¨nh', 's∆°n la', 'lai ch√¢u',
                'ƒëi·ªán bi√™n', 'h√† giang', 'ninh b√¨nh', 'nam ƒë·ªãnh', 'th√°i b√¨nh'
            ]
            
            # Mi·ªÅn Trung
            trung_provinces = [
                'thanh h√≥a', 'ngh·ªá an', 'h√† tƒ©nh', 'qu·∫£ng b√¨nh', 'qu·∫£ng tr·ªã', 'th·ª´a thi√™n hu·∫ø',
                'ƒë√† n·∫µng', 'qu·∫£ng nam', 'qu·∫£ng ng√£i', 'b√¨nh ƒë·ªãnh', 'ph√∫ y√™n', 'kh√°nh h√≤a',
                'ninh thu·∫≠n', 'b√¨nh thu·∫≠n', 'kon tum', 'gia lai', 'ƒë·∫Øk l·∫Øk', 'ƒë·∫Øk n√¥ng', 'l√¢m ƒë·ªìng'
            ]
            
            # Check provinces
            for province in bac_provinces:
                if province in location_lower:
                    return "Bac"
            
            for province in trung_provinces:
                if province in location_lower:
                    return "Trung"
            
            # Mi·ªÅn Nam (default if not B·∫Øc or Trung)
            nam_keywords = ['s√†i g√≤n', 'tp.hcm', 'h·ªì ch√≠ minh', 'ƒë·ªìng nai', 'b√¨nh d∆∞∆°ng', 'long an', 'ti·ªÅn giang', 'c·∫ßn th∆°', 'an giang']
            for keyword in nam_keywords:
                if keyword in location_lower:
                    return "Nam"
            
            return "Unknown"
        
        # Determine regions
        place_region = get_region(place_of_origin)
        addr_region = get_region(address)
        
        print(f"   üìç Qu√™ qu√°n region: {place_region}, Address region: {addr_region}")
        
        # Recommendation logic
        recommended_packages = []
        final_region = "Unknown"
        
        # Priority 1: Qu√™ qu√°n B·∫Øc/Trung ‚Üí recommend
        if place_region in ["Bac", "Trung"]:
            final_region = place_region
            recommended_packages = [
                {
                    "name": "B·∫£o hi·ªÉm thi√™n tai ng·∫≠p l·ª•t",
                    "reason": f"Qu√™ qu√°n t·∫°i mi·ªÅn {place_region} th∆∞·ªùng xuy√™n ch·ªãu ·∫£nh h∆∞·ªüng b·ªüi b√£o v√† m∆∞a l≈©. G√≥i b·∫£o hi·ªÉm n√†y b·∫£o v·ªá t√†i s·∫£n kh·ªèi thi·ªát h·∫°i do ng·∫≠p l·ª•t, l≈© qu√©t.",
                    "priority": 0.95
                },
                {
                    "name": "B·∫£o hi·ªÉm nh√† c·ª≠a tr∆∞·ªõc b√£o",
                    "reason": f"B√£o v√† gi√≥ m·∫°nh th∆∞·ªùng x·∫£y ra t·∫°i mi·ªÅn {place_region}, g√¢y h∆∞ h·∫°i cho m√°i nh√†, c·ª≠a s·ªï, t∆∞·ªùng. G√≥i n√†y ƒë·∫£m b·∫£o chi ph√≠ s·ª≠a ch·ªØa ho·∫∑c x√¢y d·ª±ng l·∫°i.",
                    "priority": 0.90
                },
                {
                    "name": "B·∫£o hi·ªÉm ph∆∞∆°ng ti·ªán ng·∫≠p n∆∞·ªõc",
                    "reason": "Xe m√°y, √¥ t√¥ d·ªÖ b·ªã ng·∫≠p n∆∞·ªõc khi m∆∞a l·ªõn ho·∫∑c l≈© l·ª•t. G√≥i n√†y gi√∫p b·ªìi th∆∞·ªùng chi ph√≠ s·ª≠a ch·ªØa ƒë·ªông c∆°, h·ªá th·ªëng ƒëi·ªán b·ªã h∆∞ h·ªèng do n∆∞·ªõc.",
                    "priority": 0.85
                }
            ]
        # Priority 2: Qu√™ qu√°n Nam + Address B·∫Øc/Trung ‚Üí recommend
        elif place_region == "Nam" and addr_region in ["Bac", "Trung"]:
            final_region = addr_region
            recommended_packages = [
                {
                    "name": "B·∫£o hi·ªÉm thi√™n tai ng·∫≠p l·ª•t",
                    "reason": f"ƒê·ªãa ch·ªâ th∆∞·ªùng tr√∫ t·∫°i mi·ªÅn {addr_region} th∆∞·ªùng xuy√™n ch·ªãu ·∫£nh h∆∞·ªüng b·ªüi b√£o v√† m∆∞a l≈©. G√≥i b·∫£o hi·ªÉm n√†y b·∫£o v·ªá t√†i s·∫£n kh·ªèi thi·ªát h·∫°i do ng·∫≠p l·ª•t, l≈© qu√©t.",
                    "priority": 0.95
                },
                {
                    "name": "B·∫£o hi·ªÉm nh√† c·ª≠a tr∆∞·ªõc b√£o",
                    "reason": f"B√£o v√† gi√≥ m·∫°nh th∆∞·ªùng x·∫£y ra t·∫°i mi·ªÅn {addr_region}, g√¢y h∆∞ h·∫°i cho m√°i nh√†, c·ª≠a s·ªï, t∆∞·ªùng. G√≥i n√†y ƒë·∫£m b·∫£o chi ph√≠ s·ª≠a ch·ªØa ho·∫∑c x√¢y d·ª±ng l·∫°i.",
                    "priority": 0.90
                },
                {
                    "name": "B·∫£o hi·ªÉm ph∆∞∆°ng ti·ªán ng·∫≠p n∆∞·ªõc",
                    "reason": "Xe m√°y, √¥ t√¥ d·ªÖ b·ªã ng·∫≠p n∆∞·ªõc khi m∆∞a l·ªõn ho·∫∑c l≈© l·ª•t. G√≥i n√†y gi√∫p b·ªìi th∆∞·ªùng chi ph√≠ s·ª≠a ch·ªØa ƒë·ªông c∆°, h·ªá th·ªëng ƒëi·ªán b·ªã h∆∞ h·ªèng do n∆∞·ªõc.",
                    "priority": 0.85
                }
            ]
        # Otherwise: No recommendation
        else:
            final_region = place_region if place_region != "Unknown" else addr_region
        
        print(f"   ‚úÖ Final region: {final_region}, Packages: {len(recommended_packages)}")
        
        return {
            "address": {
                "text": address,
                "type": "thuong_tru" if address else "unknown",
                "region": addr_region
            },
            "place_of_origin": {
                "text": place_of_origin,
                "region": place_region
            },
            "recommended_packages": recommended_packages
        }
        
    except Exception as e:
        print(f"‚ùå Error in recommend_insurance_by_person_info: {e}")
        import traceback
        traceback.print_exc()
        return {
            "error": str(e),
            "address": {
                "text": "",
                "type": "unknown",
                "region": "Unknown"
            },
            "place_of_origin": {
                "text": "",
                "region": "Unknown"
            },
            "recommended_packages": []
        }

